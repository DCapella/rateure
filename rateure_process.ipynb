{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**requests**          -> Used to recieve content from webpage.\n",
    "<br>\n",
    "**json**              -> Used to turn json files into dictionaries.\n",
    "<br>\n",
    "**PIL**               -> Used to turn file into image that can be resized.\n",
    "<br>\n",
    "**numpy**             -> Many uses, including turing PIL Image into an array.\n",
    "<br>\n",
    "**pandas**            -> Used for dataframes and dataframes manipulation.\n",
    "<br>\n",
    "**matplotlib.pyplot** -> For plotting graphs.\n",
    "<br>\n",
    "**sklearn**           -> Allows to train model from a percentage of itself.\n",
    "<br>\n",
    "**keras**             -> Used for the actual model.\n",
    "<br>\n",
    "**scripts**           -> Used to grab max likes on all photos from all of the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from scripts.get_users import GetUsers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scrapy's crawler, gathered data as XHR from multiple sources and users. Gathered all observations along with it in case there's a need for it later. In addition, gathered max like of all photos of each user in order to scale the original likes on the photos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grabs X and y from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info = []\n",
    "\n",
    "for i in range(1, 178):\n",
    "    filename = f'./data/photos-{i}.txt'\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        items = json.loads(text)\n",
    "    \n",
    "    for item in items:\n",
    "        photos_user_url = item['user']['username']\n",
    "        photos_url = item['urls']['small']\n",
    "        photos_likes = item['likes']\n",
    "        \n",
    "        photos_info.append([photos_user_url, photos_url, photos_likes])\n",
    "        \n",
    "photos_df = pd.DataFrame(photos_info, columns=['user_url', 'small_url', 'likes'])\n",
    "photos_df.to_csv('./datasets/user_url_likes.csv', index=False)\n",
    "photos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://unsplash.com/napi/users/{}/photos?page={}&per_page=10&order_by=latest'\n",
    "\n",
    "i = 1\n",
    "likes = set()\n",
    "while True:\n",
    "    url = base_url.format('gallivantinglife', i)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 404: return 0\n",
    "\n",
    "    json_file = json.loads(response.text)\n",
    "    if len(json_file) == 0:\n",
    "        print(f'broke at: {i}')\n",
    "        break\n",
    "\n",
    "    for item in json_file:\n",
    "        print(item)\n",
    "        print(item['likes'])\n",
    "        print(i)\n",
    "        likes.add(item['likes'])\n",
    "    i += 1\n",
    "    sleep(1)\n",
    "try:\n",
    "    print(max(likes))\n",
    "except:\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "photos_df.iloc[1081:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses Requests\n",
    "photos_df = pd.read_csv('./datasets/user_url_likes.csv')\n",
    "\n",
    "# Start where left off\n",
    "users_list = photos_df.iloc[1081:, 0].values.tolist()\n",
    "\n",
    "get_users = GetUsers(users_list, 'user_errors/', 'user_files/', 'users_info_3/')\n",
    "\n",
    "get_users.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Mask Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_df = pd.read_csv('./datasets/user_url_likes.csv')\n",
    "\n",
    "# First json dictionary\n",
    "file = './users_info/user_info.json'\n",
    "with open(file, 'r') as f:\n",
    "    stuff = json.loads(f.read())\n",
    "    \n",
    "# Second json dictionary\n",
    "file_2 = './users_info_2/user_info.json'\n",
    "with open(file_2, 'r') as f:\n",
    "    stuff_2 = json.loads(f.read())\n",
    "    \n",
    "# Third json dictionary\n",
    "file_3 = './users_info_3/user_info.json'\n",
    "with open(file_3, 'r') as f:\n",
    "    stuff_3 = json.loads(f.read())\n",
    "    \n",
    "# Combine the dictionaries\n",
    "stuff_2.update(stuff)\n",
    "stuff_3.update(stuff_2)\n",
    "\n",
    "transformed_likes = stuff_3.copy()\n",
    "\n",
    "# Get setup to delete any observation where profile no long exists, i.e. 0 value.\n",
    "name_delete = set()\n",
    "for key, value in transformed_likes.items():\n",
    "    if value <= 0: name_delete.add(key)\n",
    "len(name_delete)\n",
    "\n",
    "# Declare the mask for user(s) with value 0      \n",
    "mask = photos_df['user_url'].map(lambda x: x in name_delete)\n",
    "\n",
    "# Declare the index of mask\n",
    "mask_delete = photos_df[mask].index\n",
    "\n",
    "# Check and Drop\n",
    "print('Sum Before:', photos_df['user_url'].map(lambda x: x in name_delete).sum())\n",
    "check = photos_df.drop(mask_delete)\n",
    "print('Sum After:', check['user_url'].map(lambda x: x in name_delete).sum())\n",
    "proceed = input(\"Do you want to continue?\\n>>> \")\n",
    "if proceed.lower() in 'yes':\n",
    "    photos_df.drop(mask_delete, inplace=True)\n",
    "else:\n",
    "    print('Goodbye.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store\n",
    "mask_delete_df = pd.DataFrame(mask_delete, columns=['mask_delete'])\n",
    "\n",
    "mask_delete_df.to_csv('datasets/mask_delete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformed columns so we can save old y column\n",
    "photos_df['transformed_likes'] = photos_df['likes'] / photos_df['user_url'].map(transformed_likes).values\n",
    "\n",
    "# Save as official csv for CNN\n",
    "photos_df.to_csv('datasets/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photos' Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to store photo's url\n",
    "photos_info = []\n",
    "\n",
    "for i in range(1, 178):\n",
    "    filename = f'./data/photos-{i}.txt'\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        items = json.loads(text)\n",
    "    \n",
    "    for item in items:\n",
    "        photos_url = item['urls']['small']\n",
    "        photos_info.append(photos_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Requests to download photos\n",
    "for i, url in enumerate(photos_info):\n",
    "    img_filename = f'./img_small/photos-{i}.jpg'\n",
    "    img_data = requests.get(url).content\n",
    "\n",
    "    with open(img_filename, 'wb') as f:\n",
    "        f.write(img_data)\n",
    "        \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Img to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in mask_delete\n",
    "mask_delete = pd.read_csv('./datasets/mask_delete.csv')['mask_delete']\n",
    "\n",
    "# Need to grab base size in order to resize later\n",
    "img = Image.open('./img_small/photos-0.jpg')\n",
    "size = img.size\n",
    "\n",
    "# Holder will store numpy arrays of pixels\n",
    "holder = []\n",
    "\n",
    "# I know less than 18,000 but not exactly sure so stop whenever\n",
    "for i in range(2000):\n",
    "    if i not in mask_delete:\n",
    "        try:\n",
    "            # Grab each file\n",
    "            file = f'./img_small/photos-{i}.jpg'\n",
    "\n",
    "            # Open img and proceed to resize it\n",
    "            img = Image.open(file)\n",
    "            img = img.resize(size)\n",
    "\n",
    "            # Temp variable to store numpy array of pixels\n",
    "            temp = np.array(img).ravel()\n",
    "            holder.append(temp)\n",
    "        except:\n",
    "            print(f\"Broke at {i}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holder is arrays of pixels, stack to set up for CNN\n",
    "X = np.array(holder)\n",
    "\n",
    "# Only need transformed likes for y\n",
    "y = pd.read_csv('./datasets/train.csv')['transformed_likes'].values\n",
    "y = np.log10(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# # CNN\n",
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kera Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    activation='relu',\n",
    "    input_dim=X_train.shape[1]\n",
    "))\n",
    "\n",
    "model.add(Dense(\n",
    "    32,\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(\n",
    "    1,\n",
    "))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "# Only need 50 epochs\n",
    "history = model.fit(\n",
    "    X_train_ss,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_ss, y_test),\n",
    ")\n",
    "\n",
    "# Get predictions\n",
    "test_pred = model.predict(X_test_ss)\n",
    "train_pred = model.predict(X_train_ss)\n",
    "\n",
    "# Plot mse loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Testing Loss');\n",
    "plt.legend();\n",
    "\n",
    "# Plot mae loss\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['mean_absolute_error'], label='Training Loss')\n",
    "plt.plot(history.history['val_mean_absolute_error'], label='Testing Loss');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in mask_delete\n",
    "mask_delete = pd.read_csv('./datasets/mask_delete.csv')['mask_delete']\n",
    "\n",
    "# Need to grab base size in order to resize later\n",
    "img = Image.open('./img_small/photos-0.jpg')\n",
    "size = img.size\n",
    "\n",
    "# Holder will store numpy arrays of pixels\n",
    "holder = []\n",
    "\n",
    "# I know less than 18,000 but not exactly sure so stop whenever\n",
    "for i in range(2000):\n",
    "    if i not in mask_delete:\n",
    "        try:\n",
    "            # Grab each file\n",
    "            file = f'./img_small/photos-{i}.jpg'\n",
    "\n",
    "            # Open img and proceed to resize it\n",
    "            img = Image.open(file)\n",
    "            img = img.resize(size)\n",
    "\n",
    "            # Temp variable to store numpy array of pixels\n",
    "            temp = np.array(img).ravel()\n",
    "            holder.append(temp)\n",
    "        except:\n",
    "            print(f\"Broke at {i}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holder is arrays of pixels, stack to set up for CNN\n",
    "X = np.array(holder)\n",
    "\n",
    "# Only need transformed likes for y\n",
    "y = pd.read_csv('./datasets/train.csv')['transformed_likes'].values\n",
    "y = np.log10(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# # CNN\n",
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X_train)\n",
    "X_test_ss = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broke at 1770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ga/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/envs/ga/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load in mask_delete\n",
    "mask_delete = pd.read_csv('./datasets/mask_delete.csv')['mask_delete']\n",
    "\n",
    "# Need to grab base size in order to resize later\n",
    "img = Image.open('./img_small/photos-0.jpg')\n",
    "size = img.size\n",
    "\n",
    "# Holder will store numpy arrays of pixels\n",
    "holder = []\n",
    "\n",
    "# I know less than 18,000 but not exactly sure so stop whenever\n",
    "for i in range(2000):\n",
    "    if i not in mask_delete:\n",
    "        try:\n",
    "            # Grab each file\n",
    "            file = f'./img_small/photos-{i}.jpg'\n",
    "\n",
    "            # Open img and proceed to resize it\n",
    "            img = Image.open(file)\n",
    "            img = img.resize(size)\n",
    "\n",
    "            # Temp variable to store numpy array of pixels\n",
    "            temp = np.array(img).ravel()\n",
    "            holder.append(temp)\n",
    "        except:\n",
    "            print(f\"Broke at {i}\")\n",
    "            break\n",
    "\n",
    "# Holder is arrays of pixels, stack to set up for CNN\n",
    "X = np.array(holder)\n",
    "\n",
    "# Only need transformed likes for y\n",
    "y = pd.read_csv('./datasets/train.csv')['transformed_likes'].values\n",
    "y = np.log10(y)\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_ss = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1765/1765 [==============================] - 40s 23ms/step - loss: 19931.1361 - mean_absolute_error: 81.2839\n",
      "Epoch 2/12\n",
      "1765/1765 [==============================] - 38s 22ms/step - loss: 9063.9172 - mean_absolute_error: 47.8688\n",
      "Epoch 3/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 805.0770 - mean_absolute_error: 19.9670\n",
      "Epoch 4/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 377.6803 - mean_absolute_error: 13.0078\n",
      "Epoch 5/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 172.7230 - mean_absolute_error: 9.1416\n",
      "Epoch 6/12\n",
      "1765/1765 [==============================] - 38s 22ms/step - loss: 334.4969 - mean_absolute_error: 12.1415\n",
      "Epoch 7/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 407.1488 - mean_absolute_error: 14.2789\n",
      "Epoch 8/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 665.0139 - mean_absolute_error: 15.2016\n",
      "Epoch 9/12\n",
      "1765/1765 [==============================] - 38s 21ms/step - loss: 5411.2493 - mean_absolute_error: 44.0772\n",
      "Epoch 10/12\n",
      "1765/1765 [==============================] - 37s 21ms/step - loss: 2161.4030 - mean_absolute_error: 33.9435\n",
      "Epoch 11/12\n",
      "1765/1765 [==============================] - 38s 22ms/step - loss: 2271.1393 - mean_absolute_error: 34.9044\n",
      "Epoch 12/12\n",
      "1765/1765 [==============================] - 42s 24ms/step - loss: 847.6241 - mean_absolute_error: 19.2081\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(\n",
    "    128,\n",
    "    activation='relu',\n",
    "    input_dim=X_train_ss.shape[1]\n",
    "))\n",
    "\n",
    "model.add(Dense(\n",
    "    32,\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(\n",
    "    1,\n",
    "))\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer='adam',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "# Only need 12 epochs\n",
    "history = model.fit(\n",
    "    X_train_ss,\n",
    "    y,\n",
    "    epochs=12,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'kera_v_0.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
